### Risk Intelligence Dashboard 2.0 Docs

#### Running the Application
1. ssh into the web server
```shell
ssh riskintel.miserver.it.umich.edu
```
2. clone the GitHub repository (if it has not been cloned already)
```shell
git clone https://github.com/ITS-Risk-Intelligence-Dashboard-2-0/risk-intelligence-scraper.git
```
3. Create a `.env` file based on the provided `.env.template`
```shell
cp .env.template .env
```
4. Start the docker container
```shell
docker compose up --build
```
5. Navigate to the dashboard at localhost:8501 and log in

#### File Structure
##### `backend`
`core` - Default Django utils
`scheduler_api` - Django app that consist of Django REST APIs for managing scheduled tasks, running scrapers, and seeding test data. Uses django-celery-beat for periodic task scheduling and Celery for background executions.

##### `frontend`
Interface for starting / stoping the scraper, managing articles in gdrive, and calling the scheduler

##### `nginx`
Web server configuration

##### `scripts` 
`diagnose_drive.py` - Checks gdrive connectivity, authenticates and lists folders / files at the root of the gdrive and prints out each item's metadata <br>
`seed_data.py` - Resets and seed both the Django database and the shared drive folder in gdrive with test articles. Used to test the shared folder functionality. Specifically, initializes a GoogleDriveService instance, deletes all existing Articles in the Django database, creates a clean "Test Data" folder, and seeds articles. The seeding process consists of generating a dummy PDF, then that PDF is uploaded to the "Test Data" folder. Then, a new article object is created inside the database.

##### `shared/core_lib`
`articles` - Django app that defines the schema for an Article object in the database. Provides a custom delete function in views.py that will delete the article in both the gdrive and in the Postgres database.<br>
`db_utils.py` - Postgress db interfacing with the gdrive

##### `web_scraper`
`gdrive/api.py` - Contains all functionality for interacting with the gdrive through the Google API (creating folders, listing drive contents, uploading and deleting files, etc.) <br>
`maizey_api/api_call.py` - Calls the Maizey API and creates a conversation <br>
`scraper` - Automates the process of finding, filtering, categorizing, and archiving articles found on the web as PDFs into gdrive. Uses the university's AI, Maizey, to filter out irrelevant articles. <br>
`web_scraper_project` - Contains celery settings and instructions for celery to run scraper tasks<br>
`categories_config.json` - ask if autogenerated<br>
`manage.py` - Interface used by Django to run admin tests
